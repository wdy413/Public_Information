週五傍晚好！這是一個非常有意思的總結性問題，這篇 MetaFormer 論文可以看作是對過去幾年視覺 Transformer 及其變體的一次深刻反思和“返璞歸真”。

這篇論文的核心論點是：**Vision Transformer (ViT) 的成功，可能更多地歸功於其整體的宏觀架構，而不是那個複雜又昂貴的自注意力（Self-Attention）模塊本身**。

為了證明這一點，作者首先提煉出了一個名為 **MetaFormer** 的通用架構，然後證明了多種成功的模型（包括他們自己提出的極簡模型 PoolFormer）都只是這個通用架構的不同“實例化”而已。

下面我們來詳細解析圖中的各個部分。

------



### **1. MetaFormer (通用架構) - 一切的藍圖**



圖中最左側的 **MetaFormer** 是作者提煉出的一個通用藍圖（General Architecture）。它並不特指某個具體模型，而是一個**“元架構”**。

- **結構流程**: 一個標準的 MetaFormer 模塊包含兩個核心部分：
  1. **令牌混合器 (Token Mixer)**:
     - 作用：負責**token 之間的信息交互**。在視覺任務中，token 通常就是 patch embedding，所以這一步主要負責**空間維度**上的信息混合。
     - 實現：這是一個“待填空”的模塊，可以用任何能實現 token 間信息混合的操作來填充。
  2. **通道 MLP (Channel MLP)**:
     - 作用：負責**每個 token 內部的信息交互**。它對每個 token 的特徵向量獨立進行處理，混合的是**通道維度**上的信息。
     - 實現：這就是 Transformer 中常見的 FFN (Feed-Forward Network) 模塊，通常由兩個線性層（Linear Layer）和一個非線性激活函數組成。
- **宏觀結構**: 整個模塊的流程是 `Norm -> Token Mixer -> Add (殘差連接) -> Norm -> Channel MLP -> Add (殘差連接)`。作者認為，**是這個宏觀結構，而不是 Token Mixer 的具體實現，奠定了 ViT 類模型的成功基礎**。

------



### **2. 三種具體的“實例化” - 填充 Token Mixer**



圖右側的三個模型，就是用不同的模塊來填充 MetaFormer 中 “Token Mixer” 這個“坑”的具體例子。



#### **`Transformer` (例如 DeiT)**



- **Token Mixer = `Attention` (注意力)**
- **解釋**: 這是我們最熟悉的標準 Transformer 結構。它使用**多頭自注意力 (Multi-Head Self-Attention)** 作為令牌混合器。
- **特點**: `Attention` 是一種**非常強大且複雜**的混合器。它的權重是**動態生成**的，依賴於輸入數據本身（Q 和 K 的相似度），能夠捕捉長距離的、複雜的依賴關係。但它的計算成本也是三者中最高的。



#### **`MLP-like model` (例如 ResMLP)**



- **Token Mixer = `Spatial MLP` (空間 MLP)**
- **解釋**: 這類模型完全拋棄了注意力機制。它使用一個**作用於空間維度（或 patch 序列維度）的全連接層 (MLP)** 來混合 token 信息。
- **操作方式**: 對於一個維度為 `[B, N, D]` (N 是 patch 數) 的張量，它會先轉置為 `[B, D, N]`，然後用一個線性層將 `N` 維映射到 `N` 維，最後再轉置回來。
- **特點**: `Spatial MLP` 也是一種全局信息混合器，但與 `Attention` 不同，它的混合權重是**靜態的**（訓練好後就固定了），不依賴於輸入數據。它比 `Attention` 更簡單、計算更快。



#### **`PoolFormer` (本文提出的模型)**



- **Token Mixer = `Pooling` (池化)**
- **解釋**: 這是這篇論文**最大膽、最核心的貢獻**。作者提出，我們甚至不需要任何帶有可學習參數的複雜模塊，只需要一個極其簡單的**池化操作**就足夠了。
- **操作方式**: 這個 `Pooling` 操作通常是一個非常簡單的**平均池化 (Average Pooling)**，例如 `nn.AvgPool2d(kernel_size=3, stride=1, padding=1)`。它的作用僅僅是將每個 token 的特徵與其周圍鄰居的特徵進行簡單的平均混合。
- **特點**: `Pooling` 是一種**沒有任何可學習參數、極其輕量**的混合器。

------



### **結論與啟示**



`PoolFormer` 的驚人發現在於，即便用如此簡單、甚至沒有任何可學習參數的 `Pooling` 來替換掉複雜的 `Attention`，模型在 ImageNet 分類等任務上依然能取得非常有競爭力的性能。

這有力地證明了作者的核心論點：

**一個網絡的性能，很大程度上是由其宏觀架構（即 MetaFormer 結構）決定的。** 只要保留了 `Token Mixer` 和 `Channel MLP` 交替進行信息混合的範式，即使 `Token Mixer` 本身非常簡單，也能取得不錯的效果。

因此，`Transformer`, `MLP-like model`, `PoolFormer` 的關係是：**它們共享同一個名為 MetaFormer 的“骨架”，但在“心臟”（Token Mixer）部分，分別安裝了不同複雜度和成本的“引擎”（Attention, Spatial MLP, Pooling）。** 這篇論文啟發研究者們更加關注宏觀架構的設計，而不是僅僅在注意力機制上“內卷”。







周日上午好！我们接着上次的话题，深入看一下 MetaFormer 论文中提出的具体网络实现——**PoolFormer** 的完整架构。

这张图完美地展示了作者是如何将 MetaFormer 的通用理念，与一个经典的、类似 CNN 的层级式架构相结合，从而构建出一个完整且高效的视觉网络的。

------



### **1. PoolFormer 整体架构 (图 a) - 经典的层级式设计**



图 (a) 展示的是 PoolFormer 的宏观架构。如果您熟悉 Swin Transformer 或者 ResNet 等现代 CNN，您会发现这个结构非常眼熟。它的核心是**层级式（Hierarchical）**设计。

**与原始 ViT 的区别**: PoolFormer 完全抛弃了原始 ViT 那种将图像展平后，从头到尾都保持相同序列长度和维度的“扁平”结构。取而代之的是一个像金字塔一样，**空间分辨率逐层降低、通道维度逐层增加**的结构。这种设计对于视觉任务至关重要，因为它能让网络在不同尺度上学习特征。

**执行流程详解**:

1. **初始 Patch Embedding (Stem)**:
   - **输入**: `3 x H x W` 的原始图像。
   - **操作**: 这一步在图中被抽象为 `patch embedding`，但它的实际实现通常是一个**卷积主干网络 (Convolutional Stem)**。例如，通过几个带步长的 3x3 卷积层。
   - **作用**: 它的任务是将原始图像快速降采样（图中显示降了 4 倍，例如 `224x224` -> `56x56`），并提取出底层的基本特征（如边缘、颜色块），同时将通道数从 3 增加到 `C₁`（例如 48）。
   - **输出**: 一个 `C₁ x H/4 x W/4` 的特征图。
2. **四个处理阶段 (Stage 1 ~ Stage 4)**:
   - **结构**: 整个网络的主体由四个阶段串联而成。
   - **阶段内部**: 每个阶段内部，都由多个 **`PoolFormer Block`**（图 b 所示的模块）堆叠而成，用于在该分辨率下进行深度特征提取。如图注所示，一个 L 层的模型，可能会在 Stage 1, 2, 3, 4 分别堆叠 `L/6`, `L/6`, `L/2`, `L/6` 个 PoolFormer Block。
   - **阶段之间**: 连接两个阶段的是一个名为 **`patch embedding`** 的模块。**请注意**，这里的 `patch embedding`与初始的不同，它的实际作用是**降采样 (Downsampling)**，其功能与 Swin Transformer 中的 **`Patch Merging`** 完全一样。
     - **作用**: 将上一阶段的特征图空间分辨率减半（`H/4` -> `H/8`），同时将通道数翻倍（`C₁` -> `C₂`），为下一阶段提供更高级、更浓缩的语义信息。
3. **最终输出**: 经过四个阶段的处理后，会得到一个高通道、低分辨率的特征图（`C₄ x H/32 x W/32`）。这个特征图接下来可以送入一个全局平均池化层和全连接层，用于最终的图像分类。

------



### **2. PoolFormer Block (图 b) - 极简的 MetaFormer 实现**



图 (b) 展示了构成每个 Stage 的核心单元——`PoolFormer Block`。这正是我们上次讨论的 **MetaFormer 通用架构的一个极简实例**。

- **结构流程回顾**: 它完美地遵循了 MetaFormer 的 `Norm -> Token Mixer -> Add -> Norm -> Channel MLP -> Add` 的蓝图。
- **核心组件：`Pooling` 作为 Token Mixer**:
  - **作用与原理**: PoolFormer 的惊人之处在于，它将 Transformer 中那个最复杂、最昂贵的**自注意力（Self-Attention）**模块，替换成了一个极其简单的**池化（Pooling）**操作。
  - **实现方式**:
    - 这个 `Pooling` 层就是一个**不带任何可学习参数**的**平均池化层 (Average Pooling)**。
    - 例如，一个 `kernel_size=3, stride=1, padding=1` 的平均池化。对于特征图上的每一个 token（可以理解为一个像素点），这个操作会把它和它周围 8 个邻居的特征向量取一个平均值，作为这个 token 混合了邻域信息之后的新表示。
  - **实际含义**: 这是一种最简单、最直接的实现“令牌混合 (Token Mixing)”的方式。它强迫每个 token 与其紧邻的 token 进行信息交换，虽然范围很小，但通过堆叠多个这样的 block，感受野会逐渐扩大。
- **其他组件**:
  - **`Channel MLP`**: 在 `Pooling` 完成了空间维度的信息混合后，`Channel MLP` (即 FFN) 紧接着对每个 token 内部的通道维度进行信息混合。
  - **`Norm` 和 `Add`**: Layer Normalization 和残差连接保证了整个模块训练的稳定性。

------



### **总结与洞察**



PoolFormer 的完整架构向我们揭示了以下几点：

1. **宏观架构至关重要**: 一个类似 CNN 的、分层、多尺度的宏观架构，对于视觉任务来说是一个非常有效的设计范式。
2. **注意力并非不可或缺**: PoolFormer 的成功证明了，对于视觉任务，“令牌混合”不一定需要昂贵的自注意力机制来实现。即使是像 `Pooling` 这样极其简单的、无参数的局部信息混合操作，只要放在 `MetaFormer` 这样优秀的宏观结构中，也能取得非常有竞争力的性能。
3. **返璞归真**: 这个工作启发研究者们，模型的成功可能更多地来源于其整体的结构设计，而不是某个单一的、复杂的明星模块。这为设计更轻量、更高效的视觉骨干网络开辟了新的道路。









### **1. 初始 Patch Embedding 与阶段间降采样的区别**



您这个问题提得非常精准，一针见血地指出了一个容易混淆的地方。您说得没错，从结果上看，初始的 ‘Patch Embedding’ 和阶段间的降采样模块**都执行了下采样**，都会缩小特征图的尺寸。

它们的**核心区别在于其“使命”和“实现方式”**。



#### **A. 初始 Patch Embedding (通常称为 "Stem")**



- **使命**: **从“像素世界”到“特征世界”的转换器**。
  - 它的任务是接收最原始的、只有 3 个通道（RGB）的图像像素，并将这些像素信息转化成网络能够理解的、多通道的、深度的初始特征表示。它是整个网络的**入口**。
- **输入**: `[B, 3, H, W]` 的像素张量。
- **输出**: `[B, C₁, H/4, W/4]` 的特征张量。这里的 `C₁` 个通道已经是抽象的特征（例如，通过学习得到的边缘检测器、颜色检测器等），而不再是简单的 R, G, B 值。
- **实现方式**:
  - 通常是一个**更复杂的“卷积主干网络 (Convolutional Stem)”**。为了稳健地提取初始特征，它往往不止一层卷积，例如可能会用两个步长为 2 的 3x3 卷积层堆叠来实现 4 倍的下采样。



#### **B. 阶段间的降采样模块 (图中的 "patch embedding")**



- **使命**: **在“特征世界”中构建层次结构**。
  - 它的任务是接收上一阶段已经提取出的**深度特征图**，并对其进行**整合与浓缩**，以供下一阶段使用。它是在已有的特征基础上进行操作，是连接特征金字塔不同楼层的**楼梯**。
- **输入**: `[B, C_in, H', W']` 的特征张量。
- **输出**: `[B, 2*C_in, H'/2, W'/2]` 的降采样后的特征张量。
- **实现方式**:
  - 通常是一个**更简单、更标准化的操作**。正如论文所说，它的功能与 Swin Transformer 中的 `Patch Merging` 一样，可以通过“分组-拼接-线性投射”来实现，也可以直接用一个步长为 2 的 3x3 卷积来实现。

**一个比喻来总结区别**:

- **初始 Patch Embedding** 就像是**“把原木（像素）加工成标准化的木板（初始特征）”**的过程，这个过程比较复杂，需要多道工序。
- **阶段间的降采样** 就像是**“把小块木板拼接成大块的复合板”**的过程，即将已有的特征进行整合与压缩。

虽然都有“压缩”的动作，但前者的使命是**创建特征**，后者的使命是**整合特征**。

------



### **2. PoolFormer block 的数据维度变化（数值示例）**



好的，我们来用一个具体的例子，走一遍图 (b) 中 PoolFormer block 的完整数据流。

**基础设定**:

- 假设**输入**的特征图来自 Stage 2，其维度为: `[B, 64, 32, 32]` (通道 C=64, 高 H=32, 宽 W=32)。
- `Pooling` 层是一个 `kernel_size=3, stride=1, padding=1` 的平均池化。
- `Channel MLP` 的中间扩展维度是输入的 4 倍，即 `64 * 4 = 256`。

**执行流程与维度变化**:

1. **输入 (Input)**
   - 我们把输入命名为 `z_in`。
   - `z_in` 的维度: `[B, 64, 32, 32]`
2. **第一个残差分支 (Token Mixer 部分)**
   - **a. Norm**: 对 `z_in` 进行层归一化 (LayerNorm)。归一化只改变数值分布，不改变形状。
     - 输出维度: `[B, 64, 32, 32]`
   - **b. Pooling (Token Mixer)**: 对归一化后的张量进行平均池化。因为 `stride=1`，这个操作**不会改变特征图的空间尺寸**，它只是将每个像素的特征与其邻域的特征进行平均，实现局部信息混合。
     - 输出维度: `[B, 64, 32, 32]`
   - **c. Add (残差连接)**: 将 `Pooling` 的输出与原始输入 `z_in` 逐元素相加。
     - 输出维度: `[B, 64, 32, 32]`
   - 我们将这个中间结果命名为 `z_mid`。
3. **第二个残差分支 (Channel MLP 部分)**
   - **a. Norm**: 对 `z_mid` 进行层归一化。
     - 输出维度: `[B, 64, 32, 32]`
   - **b. Channel MLP (FFN)**: 对归一化后的张量进行通道混合。
     - **Linear 1 (升维)**: 通过一个 1x1 卷积将通道数从 64 扩展到 256。
       - 输出维度: `[B, 256, 32, 32]`
     - **GELU**: 非线性激活，不改变形状。
       - 输出维度: `[B, 256, 32, 32]`
     - **Linear 2 (降维)**: 通過另一個 1x1 卷積將通道數从 256 压缩回 64。
       - 输出维度: `[B, 64, 32, 32]`
   - **c. Add (残差连接)**: 将 `Channel MLP` 的输出与它的输入 `z_mid` 逐元素相加。
     - 输出维度: `[B, 64, 32, 32]`
4. **最终输出 (Final Output)**
   - 整个 PoolFormer block 的最终输出维度为 `[B, 64, 32, 32]`。

**结论**: PoolFormer block 是一个**维度保持 (Dimension-preserving)** 的模块。它的作用是在**不改变特征图宏观尺寸**的前提下，对其进行一次“空间信息混合（通过 Pooling）”和一次“通道信息混合（通过 Channel MLP）”，从而实现特征的深度提炼。

这也就解释了为什么在宏观架构中，**必须**在各个 Stage 之间插入独立的降采样模块（`Patch Merging`），因为 PoolFormer block 本身不负责降采样的任务。









周日上午好！我们接着上次的话题，深入看一下 MetaFormer 论文中提出的具体网络实现——PoolFormer 的完整架构。这两个问题都非常精准，我们来详细解析。

------



### **1. 初始 Patch Embedding 与阶段间降采样的区别**



您这个问题提得非常精准，一针见血地指出了一个容易混淆的地方。您说得没错，从结果上看，初始的 ‘Patch Embedding’ 和阶段间的降采样模块**都执行了下采样**，都会缩小特征图的尺寸。

它们的**核心区别在于其“使命”和“实现方式”**。



#### **A. 初始 Patch Embedding (通常称为 "Stem")**



- **使命**: **从“像素世界”到“特征世界”的转换器**。
  - 它的任务是接收最原始的、只有 3 个通道（RGB）的图像像素，并将这些像素信息转化成网络能够理解的、多通道的、深度的初始特征表示。它是整个网络的**入口**。
- **输入**: `[B, 3, H, W]` 的像素张量。
- **输出**: `[B, C₁, H/4, W/4]` 的特征张量。这里的 `C₁` 个通道已经是抽象的特征（例如，通过学习得到的边缘检测器、颜色检测器等），而不再是简单的 R, G, B 值。
- **实现方式**:
  - 通常是一个**更复杂的“卷积主干网络 (Convolutional Stem)”**。为了稳健地提取初始特征，它往往不止一层卷积，例如可能会用两个步长为 2 的 3x3 卷积层堆叠来实现 4 倍的下采样。



#### **B. 阶段间的降采样模块 (图中的 "patch embedding")**



- **使命**: **在“特征世界”中构建层次结构**。
  - 它的任务是接收上一阶段已经提取出的**深度特征图**，并对其进行**整合与浓缩**，以供下一阶段使用。它是在已有的特征基础上进行操作，是连接特征金字塔不同楼层的**楼梯**。
- **输入**: `[B, C_in, H', W']` 的特征张量。
- **输出**: `[B, 2*C_in, H'/2, W'/2]` 的降采样后的特征张量。
- **实现方式**:
  - 通常是一个**更简单、更标准化的操作**。正如论文所说，它的功能与 Swin Transformer 中的 `Patch Merging` 一样，可以通过“分组-拼接-线性投射”来实现，也可以直接用一个步长为 2 的 3x3 卷积来实现。

**一个比喻来总结区别**:

- **初始 Patch Embedding** 就像是**“把原木（像素）加工成标准化的木板（初始特征）”**的过程，这个过程比较复杂，需要多道工序。
- **阶段间的降采样** 就像是**“把小块木板拼接成大块的复合板”**的过程，即将已有的特征进行整合与压缩。

虽然都有“压缩”的动作，但前者的使命是**创建特征**，后者的使命是**整合特征**。

------



### **2. PoolFormer block 的数据维度变化（数值示例）**



好的，我们来用一个具体的例子，走一遍图 (b) 中 PoolFormer block 的完整数据流。

**基础设定**:

- 假设**输入**的特征图来自 Stage 2，其维度为: `[B, 64, 32, 32]` (通道 C=64, 高 H=32, 宽 W=32)。
- `Pooling` 层是一个 `kernel_size=3, stride=1, padding=1` 的平均池化。
- `Channel MLP` 的中间扩展维度是输入的 4 倍，即 `64 * 4 = 256`。

**执行流程与维度变化**:

1. **输入 (Input)**
   - 我们把输入命名为 `z_in`。
   - `z_in` 的维度: `[B, 64, 32, 32]`
2. **第一个残差分支 (Token Mixer 部分)**
   - **a. Norm**: 对 `z_in` 进行层归一化 (LayerNorm)。归一化只改变数值分布，不改变形状。
     - 输出维度: `[B, 64, 32, 32]`
   - **b. Pooling (Token Mixer)**: 对归一化后的张量进行平均池化。因为 `stride=1`，这个操作**不会改变特征图的空间尺寸**，它只是将每个像素的特征与其邻域的特征进行平均，实现局部信息混合。
     - 输出维度: `[B, 64, 32, 32]`
   - **c. Add (残差连接)**: 将 `Pooling` 的输出与原始输入 `z_in` 逐元素相加。
     - 输出维度: `[B, 64, 32, 32]`
   - 我们将这个中间结果命名为 `z_mid`。
3. **第二个残差分支 (Channel MLP 部分)**
   - **a. Norm**: 对 `z_mid` 进行层归一化。
     - 输出维度: `[B, 64, 32, 32]`
   - **b. Channel MLP (FFN)**: 对归一化后的张量进行通道混合。
     - **Linear 1 (升维)**: 通过一个 1x1 卷积将通道数从 64 扩展到 256。
       - 输出维度: `[B, 256, 32, 32]`
     - **GELU**: 非线性激活，不改变形状。
       - 输出维度: `[B, 256, 32, 32]`
     - **Linear 2 (降维)**: 通過另一個 1x1 卷積將通道數从 256 压缩回 64。
       - 输出维度: `[B, 64, 32, 32]`
   - **c. Add (残差连接)**: 将 `Channel MLP` 的输出与它的输入 `z_mid` 逐元素相加。
     - 输出维度: `[B, 64, 32, 32]`
4. **最终输出 (Final Output)**
   - 整个 PoolFormer block 的最终输出维度为 `[B, 64, 32, 32]`。

**结论**: PoolFormer block 是一个**维度保持 (Dimension-preserving)** 的模块。它的作用是在**不改变特征图宏观尺寸**的前提下，对其进行一次“空间信息混合（通过 Pooling）”和一次“通道信息混合（通过 Channel MLP）”，从而实现特征的深度提炼。

这也就解释了为什么在宏观架构中，**必须**在各个 Stage 之间插入独立的降采样模块（`Patch Merging`），因为 PoolFormer block 本身不负责降采样的任务。











周日上午好！我们接着上次的话题，深入看一下 MetaFormer 论文中提出的具体网络实现——PoolFormer 的完整架构。

这张表格是 PoolFormer 论文的核心成果展示之一，它非常清晰地定义了从 S12（Small-12 layers）到 M48（Medium-48 layers）等不同大小的 PoolFormer 模型的详细**“配方”**。理解这张表，就能完全复现出作者提出的所有模型。

下面我们来详细解读这张表的内容。

------



### **1. 解读表格结构与命名规则**



这张表描述了如何像搭积木一样，用标准化的组件搭建起一个完整的 PoolFormer 网络。



#### **表格的行 (Rows) - 网络的四个阶段**



- **`Stage 1` 到 `Stage 4`**: 表格的主体是按照我们之前讨论过的四个层级阶段来组织的。
- **`#Tokens`**: 这一列显示了每个阶段输出的特征图的空间分辨率。可以看到，分辨率从 `H/4 x W/4` 逐级减半，到 `H/32 x W/32` 结束，形成了一个特征金字塔。
- **`Layer Specification`**: 这一部分定义了每个阶段内部的两种核心组件：
  - **`Patch Embedding`**: 这是位于**阶段之间**的**降采样模块**。
  - **`PoolFormer Block`**: 这是每个阶段**内部**进行特征提取的核心模块。



#### **表格的列 (Columns) - 不同的模型规模**



- **`S12`, `S24`, `S36`, `M36`, `M48`**: 这些是作者提出的不同大小的模型名称。



#### **关键的命名规则**



这个命名非常直观，由**字母**和**数字**组成：

1. **字母 `S` vs `M`**: 代表模型的**宽度**，即通道数（`Embedding Dim.`）的规模。
   - **`S` (Small)**: 采用较小的通道数配置，四个阶段的输出通道数分别为 `64, 128, 320, 512`。
   - **`M` (Medium)**: 采用中等的通道数配置，四个阶段的输出通道数分别为 `96, 192, 384, 768`。
2. **数字 `12, 24, 36, 48`**: 代表模型中 **`PoolFormer Block` 的总层数**。
   - 例如，**`S24`** 就意味着这是一个采用**小尺寸通道数**配置、并且总共包含 **24** 个 `PoolFormer Block` 的模型。

------



### **2. 以 `PoolFormer-S24` 为例，进行一次完整的“走读”**



为了让您有更直观的感受，我们来详细追踪一下 `PoolFormer-S24` 模型的数据流和维度变化。

**前提**: 输入图像为 `[B, 3, 224, 224]`。

1. **初始 Stem (表中未显示)**:
   - 图像首先通过一个卷积主干网络（Stem），进行第一次 4 倍下采样。
   - **输出**: `[B, 64, 56, 56]`。（因为是 `S` 模型，所以 Stage 1 的输入通道数为 64）
2. **Stage 1**:
   - **输入**: `[B, 64, 56, 56]`
   - **处理**: 查表可知，S24 在 Stage 1 有 **4** 个 `PoolFormer Block`。这些 block 内部进行特征提取，但**不改变**特征图的尺寸。
   - **输出**: `[B, 64, 56, 56]`
3. **降采样 (进入 Stage 2)**:
   - **操作**: 使用 Stage 2 定义的 `Patch Embedding` 模块（一个 `3x3, stride=2` 的卷积或等效操作）。
   - **处理**: 空间分辨率减半，通道数增加到 Stage 2 的 `Embed. Dim.`，即 128。
   - **输出**: `[B, 128, 28, 28]`
4. **Stage 2**:
   - **输入**: `[B, 128, 28, 28]`
   - **处理**: 查表可知，S24 在 Stage 2 有 **4** 个 `PoolFormer Block`。
   - **输出**: `[B, 128, 28, 28]`
5. **降采样 (进入 Stage 3)**:
   - **操作**: 使用 Stage 3 定义的 `Patch Embedding` 模块。
   - **处理**: 分辨率减半，通道数增加到 Stage 3 的 `Embed. Dim.`，即 320。
   - **输出**: `[B, 320, 14, 14]`
6. **Stage 3**:
   - **输入**: `[B, 320, 14, 14]`
   - **处理**: 查表可知，S24 在 Stage 3 有 **12** 个 `PoolFormer Block`。这是网络的主体，进行了最深度的特征提取。
   - **输出**: `[B, 320, 14, 14]`
7. **降采样 (进入 Stage 4)**:
   - **操作**: 使用 Stage 4 定义的 `Patch Embedding` 模块。
   - **处理**: 分辨率减半，通道数增加到 Stage 4 的 `Embed. Dim.`，即 512。
   - **输出**: `[B, 512, 7, 7]`
8. **Stage 4**:
   - **输入**: `[B, 512, 7, 7]`
   - **处理**: 查表可知，S24 在 Stage 4 有 **4** 个 `PoolFormer Block`。
   - **输出**: `[B, 512, 7, 7]`
9. **分类头 (表中未显示)**:
   - 最终 `[B, 512, 7, 7]` 的特征图会经过一个全局平均池化和全连接层，得到最终的分类结果。

- **验证总层数**: `4 (S1) + 4 (S2) + 12 (S3) + 4 (S4) = 24` 个 PoolFormer blocks，与 `S24` 的命名完全吻合。

------



### **3. 其他关键参数解读**



- **`Pooling Size`**: 在所有 `PoolFormer Block` 中，令牌混合器（Token Mixer）都使用 `3x3, stride=1` 的池化。这再次确认了 PoolFormer block 本身不进行降采样，只负责局部信息混合。
- **`MLP Ratio`**: 在所有 `PoolFormer Block` 中，`Channel MLP` 的扩展比例都是 4。这是一个继承自 Transformer 的经典设计，即 FFN 的中间维度是输入维度的 4 倍。
- **`Parameters (M)` 和 `MACs (G)`**: 表格最下方显示了每个模型的总参数量（单位：百万 M）和计算量（单位：十亿 G MACs）。这些是衡量模型大小和速度的核心指标，可以清晰地看到从 S12 到 M48，模型的复杂度和规模在不断提升。
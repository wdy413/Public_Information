简单来说，34 层的 ResNet 和 50 层的 ResNet，它们最核心的区别在于使用了两种**不同结构的残差模块 (Building blocks)**。

- **ResNet-34** (以及更浅的 ResNet-18) 使用的是 **“基础模块” (Basic Block)**。
- **ResNet-50** (以及更深的 101, 152) 使用的是 **“瓶颈模块” (Bottleneck Block)**。

下面我们来详细解析这两种模块的区别和各自的作用。

------



### **1. ResNet-34 的基础模块 (Basic Block)**



- **结构**: 请看表格中 "34-layer" 这一列的 `conv2.x` 到 `conv5.x`。它的结构非常直接，就是一个由**两个 `3x3` 卷积层**串联而成的残差单元。

  **数据流**: `Input -> Conv 3x3 -> BN -> ReLU -> Conv 3x3 -> BN -> (+) -> ReLU` *(注：`+` 代表将此路徑的输出与原始输入相加)*

- **特点**: 结构简单直观，通过两个 `3x3` 卷积层来学习一个残差映射。对于中等深度的网络（如 18 层、34 层），这种结构非常有效。

------



### **2. ResNet-50 的瓶颈模块 (Bottleneck Block)**



- **结构**: 请看表格中 "50-layer" 这一列。它的结构更为复杂，由 **`1x1`，`3x3`，`1x1` 三个卷积层**串联而成。

  **数据流**: `Input -> Conv 1x1 -> BN -> ReLU -> Conv 3x3 -> BN -> ReLU -> Conv 1x1 -> BN -> (+) -> ReLU`

- **核心思想：“瓶颈”** 这个结构的设计非常巧妙，形成了一个“**宽 -> 窄 -> 宽**”的通道变化模式，中间的 `3x3` 卷积层处于一个通道数较少的“瓶颈”位置。 我们以 `conv4.x` 阶段为例：

  1. **第一个 `1x1` 卷积 (降维)**: 它的输入通道数是 `1024`，但输出通道数只有 `256`。它负责将特征图的通道数“压窄”，**减少后续 3x3 卷积的计算量**。
  2. **中间的 `3x3` 卷积**: 它在一个“窄”的 `256` 通道上进行空间特征提取。这是整个模块中计算成本最高的部分，但由于通道数被压缩了，其计算量远小于直接在 `1024` 通道上操作。
  3. **最后的 `1x1` 卷积 (升维)**: 它负责将通道数从 `256` 恢复并提升到 `1024`，以匹配残差连接的另一端。

------



### **3. 为什么需要“瓶颈模块”？—— 效率的权衡**



既然基础模块更简单，为什么在构建更深的网络（50 层及以上）时要换成更复杂的瓶颈模块呢？

**核心原因：为了在网络不断加深的同时，保持计算效率和控制参数数量。**

我们来做一个简单的计算对比，假设我们要构建一个输入和输出通道数都为 `256` 的残差块：

- **使用两个 `3x3` 的基础模块**:
  - 第一个 `3x3` 卷积的参数量: `256 (in) x 256 (out) x 3 x 3 = 589,824`
  - 第二个 `3x3` 卷积的参数量: `256 (in) x 256 (out) x 3 x 3 = 589,824`
  - **总参数量 ≈ 118 万**
- **使用 `1x1 -> 3x3 -> 1x1` 的瓶颈模块** (瓶颈通道数为 64):
  - 第一个 `1x1` 卷积 (降维): `256 (in) x 64 (out) x 1 x 1 = 16,384`
  - 中间的 `3x3` 卷积: `64 (in) x 64 (out) x 3 x 3 = 36,864`
  - 最后的 `1x1` 卷积 (升维): `64 (in) x 256 (out) x 1 x 1 = 16,384`
  - **总参数量 ≈ 6.9 万**

**结论**: 可以看到，对于相同输入输出维度的模块，**瓶颈模块的参数量和计算量远小于基础模块**。这种设计使得研究者能够将网络堆叠到非常深（50, 101, 152 层甚至更深），同时模型的复杂度和计算成本仍然在一个可控的范围内。



### **总结对比**



| 对比项       | **34-Layer (基础模块)** | **50-Layer (瓶颈模块)**           |
| ------------ | ----------------------- | --------------------------------- |
| **结 构**    | 两个 `3x3` 卷积         | `1x1` -> `3x3` -> `1x1` 卷积      |
| **核心思想** | 直接学习残差            | **先降维，再处理，后升维**        |
| **优 點**    | 结构简单                | **计算效率极高，参数更少**        |
| **适用场景** | 中等深度的网络 (≤ 34层) | **需要构建非常深的网络 (≥ 50层)** |

导出到 Google 表格

简单来说，**瓶颈模块是用更复杂的设计，换取了更高的计算效率**，是构建现代深度卷积神经网络的基石。






[toc]









1.算法层面稀疏重排

由于要保证数学等价性，输入tensor需要做完全一致的重排。在一次计算中需要对输入做不规则的内存读取和写入，会极大降低内存带宽利用率。并且寻找最优重排方案是NPhard问题。

2.NVIDIA安培架构2:4优势，吞吐量、有效算力翻倍

比例需要保证2的幂次，非2的幂次比如2:6是3倍，设计上不容易实现；2:8是4倍25%的稀疏度精度损失太大。相比1:2的小粒度，更具有局部灵活性；相比于大粒度4:8可能会导致某些重要的神经元连接被过度剪枝，并且C84需要的编码位数更多，2:4位置编码信息元数据C42只需要3bit。

需要在算法层面进行稀疏化的时候严格保证。工程化的结论现有的剪枝算法都是理论上找不损失精度的最大稀疏化不太实用。

3.docker常用命令

```shell
docker build -t my-app:1.0 .//在dockerfile路径下执行，构建一个tag为my-app:1.0的images
docker images
docker ps

docker start <容器ID或容器名>
docker stop <容器ID或容器名>
docker restart <容器ID或容器名>
docker rm <容器ID或容器名> //必须先停止容器才能删除
docker exec -it <容器ID或容器名> /bin/bash
```



4.MAC利用率排查

在训练时，可以打开一个新的终端，输入`nvidia-smi -l 1`，持续观察GPU的利用率（GPU-Util）。如果利用率长时间低于90%，而`s/it`值又很大，那么瓶颈很可能在数据I/O或CPU端。





































conda打开sigmastar的虚拟环境后，安装一些深度学习依赖，我的python版本是3.10，需要安装的torch版本是，onnx的版本是



我现在想对我的python文件通过调用argparse实现终端信息的输入，但现在存在问题，比如我希望通过终端传入模型的名称比如onnx_tv_resnet18，然后我希望程序中执行from onnx_tv_resnet18.py import onnx_preprocess，能实现吗用python代码简单举例。另外为什么有时候传参是双横线--，有时是单横线-



我现在需要实现一个脚本all.sh。

我当前文件夹结构是：

 ~/Synchronize/onnx_tv_resnet18/ tree

**.**

├── onnx_tv_resnet18.onnx

├── onnx_tv_resnet18.py

└── **run_sh**

  ├── 1.sh

  ├── 2.sh

  ├── 3.sh

  ├── 4.sh

  ├── all.sh

  ├── get_model.py

  ├── **out_put**

  └── run_dataset.py

该脚本现在已经实现从绝对路径中脚本当前路径run_sh的上一级地址parent_dir，取得模型名称model_name，此处为onnx_tv_resnet18

现在需要完善该脚本实现以下功能：

1.把上一级路径parent_dir中的onnx_tv_resnet18.py，即${model_name}.py放入run_sh中

2.执行当前路径中的get_model.py，即python3 get_model.py --input model ${model_name}

3.等待上一步执行完成后，执行当前路径中的run_dataset.py，即python3 run_dataset.py --input model ${model_name}

4.等待上一步执行完成后执行一个命令行的python3（对应原来1.sh的内容，我现在直接全部复制到all.sh中），这里不需要关注，随便写一些内容做占位符即可

5.从out_put文件夹中取出一个txt文件放入当前路径run_sh下，这个txt文件是out_put中唯一的txt文件，并且文件夹的命名中包含模型名称${model_name}

6.删除run_sh中的onnx_tv_resnet18.py













[Revisiting Pruning at Initialization Through the Lens of Ramanujan Graph](https://openreview.net/forum?id=uVcDssQff_)

好的，我用一个更通俗易懂的方式为你解释这篇论文的核心思想。

简单来说，这篇论文研究的是一种叫做 **“初始化剪枝” (Pruning at Initialization, PaI)** 的神经网络压缩技术。

------



### 背景：什么是神经网络剪枝？



想象一下，一个神经网络就像一个极其复杂的大脑，里面有数百万甚至数十亿个神经元连接。但其中很多连接其实是“滥竽充数”的，贡献不大。**剪枝 (Pruning)** 的目的就是像园丁修剪树木一样，把这些多余的、不重要的连接剪掉，从而得到一个更小、更快、更省电的网络，同时还希望它的性能（比如识别图像的准确率）不会下降太多。

- **传统剪枝（训练后剪枝）**：先把这个庞大的网络完整地训练好（非常耗时耗力），然后再回头分析哪些连接不重要，再把它们剪掉。这就像盖好一整栋摩天大楼后，再进去把不需要的墙敲掉。效果好，但前期浪费严重。
- **初始化剪枝 (PaI)**：这是这篇论文的重点。它试图在**训练开始之前**，就“慧眼识珠”，直接找出哪些连接是“潜力股”，只保留这些有潜力的连接构成一个稀疏的“子网络”，然后只训练这个小网络。这就像在画蓝图时就设计好一个精简高效的结构，直接照着盖。如果能成功，将从头到尾节省大量计算资源。

------



### 作者发现了什么问题？



之前，学术界有一种很流行的理论，认为一个好的稀疏网络，应该像一种叫做 **“拉马努金图” (Ramanujan Graph)** 的数学结构。

你可以把“拉马努金图”理解为一种**“极致的连接典范”**：它用的连接非常少（**稀疏**），但信息在其中流动却极其高效（**高度连通**）。这听起来简直就是神经网络剪枝的完美目标，对吧？

然而，这篇论文的作者通过实验发现，**这个理论和实际情况对不上号**。

1. **不相关**：一个剪枝后的网络在多大程度上“像”拉马-努金图，跟它最终的性能好坏**没有明显的关系**。
2. **会跑偏**：如果硬是追求让网络变得最像拉马努金图，反而可能得到一个没什么结构意义的、像**随机乱连**一样的网络。

作者指出，根本原因在于“拉马努金图”的数学定义太苛刻了，它对网络的一个关键属性（某个特征值）的限制太死，不适用于神经网络这种高度稀疏的场景。

------



### 作者提出了什么解决方案？



既然老的衡量标准（拉马努金图）不好用，作者就提出了两个新的、更有效的分析工具：

1. **IMDB (Iterative Mean Difference of Bound)**
   - **作用**：这是一个**“优等生”指标**。它放宽了拉马努金图那个过于严格的条件，能更准确地衡量一个稀疏网络的连接质量是不是真的好。
   - **结论**：作者发现，**IMDB 指标越高的子网络，训练后的性能也确实越好**。这说明 IMDB 是一个有效的“指挥棒”，可以指导我们如何剪枝。
2. **NaRC (Normalized Random Coefficient)**
   - **作用**：这是一个**“警戒线”指标**。它能告诉我们，当网络被剪得太稀疏时，它的连接结构在什么时候会退化成“纯粹的随机乱连”，从而失去学习的意义。
   - **结论**：NaRC 划定了一个边界，帮助我们避免剪枝过度，掉入“随机陷阱”。

------



### 结论与意义



总而言之，这篇论文做出了以下贡献：

- **纠正了一个误区**：证明了“拉马努金图”并非衡量初始化剪枝好坏的黄金标准。
- **提供了两个新工具**：提出了 **IMDB** 和 **NaRC** 这两个新指标。IMDB 告诉我们**努力的方向**（什么样的连接更好），而 NaRC 告诉我们**底线在哪里**（不能比随机更差）。
- **指明了未来方向**：利用这两个新工具，研究人员可以更深刻地理解不同剪枝方法背后的原理，并设计出更优秀的剪枝算法，最终目标是用更少的计算资源训练出同样强大的 AI 模型。🧠✨



[Bit-Pruning: A Sparse Multiplication-Less Dot-Product](https://openreview.net/forum?id=YUDiZcZTI8)

根据这篇摘要，文章的主要贡献可以总结为以下几点：

------



### ## 核心问题



神经网络中的核心运算是**点积 (dot-product)**，而点积运算依赖大量的**乘法 (`\*`)**。乘法运算非常消耗计算能力和电能，这使得在手机、手表等资源有限的边缘设备上部署复杂的 AI 模型变得十分困难。

------



### ## 创新的解决方案



作者提出了一个巧妙的方法来**消除乘法运算**，从而构建更节能的神经网络。

1. **用加法 (`+`) 和位移 (`<<`) 替代乘法 (`\*`)**：他们发现，任何一个整数权重与激活值的乘法，都可以被分解为一系列的**加法和位移**操作。
   - **简单例子**：计算 `5 * x`。数字 5 的二进制是 `101`。这个乘法可以被改写为 `(x << 2) + (x << 0)`，也就是 `4*x + 1*x`。这样，原本一个耗能的乘法运算，就变成了一个更节能的加法和两个几乎不耗能的位移运算。
2. **关键洞察**：在这种新方法下，所需的**加法次数**完全取决于权重值的二进制表示中有多少个 **“1”**。例如，`5` (二进制 `101`) 需要 2 次加法，而 `4` (二进制 `100`) 只需要 1 次加法。

------



### ## 主要贡献：提出“比特剪枝” (Bit-Pruning)



基于以上洞察，作者提出了一种全新的神经网络压缩技术——**比特剪枝 (Bit-Pruning)**。

- **这是什么？**：传统的剪枝方法是“权重剪枝” (Weight-Pruning)，即直接将不重要的整个权重参数删除（设为 0）。而“比特剪枝”更精细，它不删除整个权重，而是**修剪权重二进制表示中的 “1”**，尽可能地将它们变成 “0”。
- **为什么有效？**：每将一个 “1” 变成 “0”，就意味着在实际运算中**减少了一次加法操作**，从而直接降低了能耗。作者称其为一种“软性”的权重剪枝，因为它不是粗暴地移除整个权重，而是精细地优化其内部结构。

------



### ## 实验结果与意义



作者通过大量实验证明，使用“比特剪枝”训练出的**无乘法稀疏网络**，在**“准确率-能耗”的权衡**上，表现优于使用传统“权重剪枝”训练出的网络。

这意味着，在达到相同准确率的情况下，比特剪枝的模型更省电；或者在同等能耗下，它的准确率更高。这项工作为在资源受限设备上部署高效、强大的 AI 模型提供了一条极具潜力的新路径。💡🔋



[A Unified Framework for Soft Threshold Pruning](https://openreview.net/forum?id=cCFqcrq0d8)

好的，根据这篇摘要，这篇文章的主要贡献可以概括为以下几点：

------



### ## 核心问题



“软阈值剪枝”是一种非常先进、效果很好的神经网络压缩技术。但之前的方法在如何设置“剪枝阈值”（即决定一个参数是否被剪掉的标准）上，存在两个问题：

1. **缺乏理论**：要么像没头苍蝇一样盲目地搜索一个好的调整方案，要么干脆让阈值自己“学”，但没人能从一个统一的理论角度解释清楚为什么要这么做。
2. **方法混乱**：各种调整阈值的方法看起来五花八门，没有统一的范式。

------



### ## 本文的核心贡献



这篇文章最大的贡献是为“软阈值剪枝”这个领域提供了**一个坚实、统一的理论框架**，把“玄学调参”变成了“科学方法”。

1. **全新的理论视角**
   - 作者巧妙地将软阈值剪枝问题，重新定义为一个经典的数学优化问题，并指出这个问题可以用一个叫做 **ISTA (迭代收缩阈值算法)** 的成熟方法来求解。
   - 这相当于为剪枝技术找到了强大的数学“靠山”，让整个过程变得有理可依。
2. **统一并解释了过去的方法**
   - 在这个新的理论框架下，作者证明了以前所有看似不同的阈值调整策略，其**本质都是在调整同一个东西**：L1 正则化项（一种鼓励模型稀疏的数学工具）。
   - 这一下子就统一了该领域的各种方法，让人们明白了它们背后的共同原理。
3. **推导出了更优的新方法**
   - 作者不只停留在解释过去，他们利用这个理论框架，推导出了一个**最优的阈值调整方案**。
   - 这个新方案的核心是让优化目标（L1 正则化系数）在整个训练过程中保持**稳定**，这在优化理论上是更合理、更优雅的做法。
4. **强大的性能和通用性**
   - **效果顶尖**：实验证明，他们提出的新算法在多种主流神经网络（如 ResNet, MobileNet）和新兴的脉冲神经网络（SNN）上，都达到了**业界顶尖 (State-of-the-art)** 的性能。
   - **通用性强**：这个理论框架非常强大，不仅能用于常规的“边训练边剪枝”，还能轻松衍生出一整套**剪枝方法家族**，包括“早期剪枝”和“初始化剪枝”等不同场景。

------



### ## 总结



简单来说，这篇论文的贡献就是：**为软阈值剪枝找到了一个“大一统”的理论基础 (ISTA 框架)，不仅解释了过去所有方法的本质，还基于该理论创造出了一个性能更强、更科学的新方法。** 👑







[DFPC: Data flow driven pruning of coupled channels without data](https://openreview.net/forum?id=mhnHqRqcjYU)

根据这篇摘要，这篇文章的主要贡献可以概括如下：

------



### ## 核心问题



文章旨在解决一个神经网络剪枝领域里的开放性难题：如何在**“数据无关” (data-free)** 的情况下，进行**结构化剪枝**。

- **结构化剪枝**：不是修剪零散的权重参数，而是直接剪掉整个滤波器 (filter) 或通道，这样能真正实现硬件加速。
- **数据无关**：在无法访问原始训练数据集和损失函数的情况下进行剪枝。这在现实中非常重要，因为数据可能涉及隐私或无法获取。

------



### ## 核心假设与新范式



为了解决这个问题，作者提出了一系列创新性的想法和方法：

1. **提出了“判别性滤波器假说” (Discriminative Filters Hypothesis)**
   - **核心思想**：一个训练得好的神经网络，其内部的滤波器必然有一部分是**“判别性”**的（能有效区分不同类别），而另一部分则是**“非判别性”**的。作者认为，这些“非判别性”的滤波器是冗余的，可以被安全地剪掉，而不会影响模型的预测性能。
2. **开创了“分布剪枝” (Distributional Pruning) 的新范式**
   - **解决方案**：作者放宽了“完全无数据”的苛刻条件，提出我们不需要原始的训练数据，**只需要能生成与原始数据同分布的数据即可**。例如，我们不需要原始的猫狗图片集，只需要另外找一些猫和狗的图片就可以进行剪枝。这在实践中更容易实现。

------



### ## 主要贡献与创新点



1. **量化“判别能力”的数学工具**
   - 为了将上述假说付诸实践，作者需要一个方法来衡量一个滤波器到底有多“判别性”。
   - 他们创造性地使用了 **“全变差 (Total Variation, TV) 距离”** 这个数学工具。具体来说，他们观察一个滤波器在输入不同类别（比如“猫”和“狗”）的图片时，其输出分布的差异有多大。**差异越大（TV 距离越大），说明这个滤波器的判别能力越强。**
2. **全新的剪枝算法：TVSPrune**
   - 这是本文最主要的贡献。作者基于 TV 距离设计了一个**全新的、一次性完成的剪枝算法，名为 TVSPrune**。
   - 该算法计算每个滤波器的判别能力得分，然后将得分最低的“非判别性”滤波器一次性剪掉。
   - 他们还将其扩展为一个迭代版本 **IterTVSPrune**，通过多次小步剪枝来达到更高的稀疏度。
3. **预测可剪枝性的指标：LDIFF 分数**
   - 作者还定义了一个名为 **LDIFF** 的分数，用来评估网络中的某一层有多少“非判别性”的滤波器。这个分数可以帮助预测这一层有多大的潜力被剪枝。

------



### ## 实验结果与意义



- **效果显著**：实验证明，TVSPrune 算法非常有效。在某些情况下，它可以在**不进行任何微调**的情况下，剪掉模型 **60% 的参数**，而准确率仅下降 **2%**。
- **超越前人**：该方法的性能远超之前的基线方法，优势接近 **10%**。

总而言之，这篇论文为“数据无关”场景下的结构化剪枝问题，提供了一套从**核心假说、理论范式、数学度量到全新算法**的完整解决方案，并用出色的实验结果证明了其有效性。🏆



[TVSPrune - Pruning Non-discriminative filters via Total Variation separability of intermediate representations without fine tuning](https://openreview.net/forum?id=sZI1Oj9KBKy)

根据这篇摘要，这篇文章的主要贡献可以概括如下：

------



### ## 核心问题



文章旨在解决一个神经网络剪枝领域里的开放性难题：如何在**“数据无关” (data-free)** 的情况下，进行**结构化剪枝**。

- **结构化剪枝**：不是修剪零散的权重参数，而是直接剪掉整个滤波器 (filter) 或通道，这样能真正实现硬件加速。
- **数据无关**：在无法访问原始训练数据集和损失函数的情况下进行剪枝。这在现实中非常重要，因为数据可能涉及隐私或无法获取。

------



### ## 核心假设与新范式



为了解决这个问题，作者提出了一系列创新性的想法和方法：

1. **提出了“判别性滤波器假说” (Discriminative Filters Hypothesis)**
   - **核心思想**：一个训练得好的神经网络，其内部的滤波器必然有一部分是**“判别性”**的（能有效区分不同类别），而另一部分则是**“非判别性”**的。作者认为，这些“非判别性”的滤波器是冗余的，可以被安全地剪掉，而不会影响模型的预测性能。
2. **开创了“分布剪枝” (Distributional Pruning) 的新范式**
   - **解决方案**：作者放宽了“完全无数据”的苛刻条件，提出我们不需要原始的训练数据，**只需要能生成与原始数据同分布的数据即可**。例如，我们不需要原始的猫狗图片集，只需要另外找一些猫和狗的图片就可以进行剪枝。这在实践中更容易实现。

------



### ## 主要贡献与创新点



1. **量化“判别能力”的数学工具**
   - 为了将上述假说付诸实践，作者需要一个方法来衡量一个滤波器到底有多“判别性”。
   - 他们创造性地使用了 **“全变差 (Total Variation, TV) 距离”** 这个数学工具。具体来说，他们观察一个滤波器在输入不同类别（比如“猫”和“狗”）的图片时，其输出分布的差异有多大。**差异越大（TV 距离越大），说明这个滤波器的判别能力越强。**
2. **全新的剪枝算法：TVSPrune**
   - 这是本文最主要的贡献。作者基于 TV 距离设计了一个**全新的、一次性完成的剪枝算法，名为 TVSPrune**。
   - 该算法计算每个滤波器的判别能力得分，然后将得分最低的“非判别性”滤波器一次性剪掉。
   - 他们还将其扩展为一个迭代版本 **IterTVSPrune**，通过多次小步剪枝来达到更高的稀疏度。
3. **预测可剪枝性的指标：LDIFF 分数**
   - 作者还定义了一个名为 **LDIFF** 的分数，用来评估网络中的某一层有多少“非判别性”的滤波器。这个分数可以帮助预测这一层有多大的潜力被剪枝。

------



### ## 实验结果与意义



- **效果显著**：实验证明，TVSPrune 算法非常有效。在某些情况下，它可以在**不进行任何微调**的情况下，剪掉模型 **60% 的参数**，而准确率仅下降 **2%**。
- **超越前人**：该方法的性能远超之前的基线方法，优势接近 **10%**。

总而言之，这篇论文为“数据无关”场景下的结构化剪枝问题，提供了一套从**核心假说、理论范式、数学度量到全新算法**的完整解决方案，并用出色的实验结果证明了其有效性。🏆







[OTOv2: Automatic, Generic, User-Friendly](https://openreview.net/forum?id=7ynoX1ojPMt)

根据这篇摘要，这篇文章的核心贡献是提出了一个名为 **OTOv2 (Only-Train-Once v2)** 的全自动、一站式神经网络压缩框架，它极大地简化了结构化剪枝的过程。

------



### ## 核心问题



现有的模型压缩（结构化剪枝）方法通常**过于复杂和繁琐**。它们就像一个需要多道工序的手工作坊：

- **多阶段流程**：用户通常需要经历“训练 → 剪枝 → 微调 → 再剪枝 → 再微调……”等多个繁琐的步骤。
- **需要专家知识**：每一步都需要用户具备大量的工程经验和领域知识来进行手动调整和干预。
- **通用性差**：这些方法往往是为特定模型（如 VGG）设计的，很难直接应用到其他更复杂的网络结构上。

这些缺点严重阻碍了剪枝技术在更广泛场景中的应用。

------



### ## 本文的核心贡献：OTOv2 —— “一键式”智能压缩



OTOv2 旨在解决上述所有痛点，其核心贡献是实现**“只训练一次”**的全自动压缩。它就像一个全自动的智能工厂，用户只需按下启动按钮即可。

1. **极致的自动化与易用性**
   - **一站式完成**：OTOv2 可以在**从零开始训练模型的同时，自动完成压缩**。整个过程只需一次，最终直接产出一个紧凑且性能优异的模型。
   - **无需微调**：最关键的是，这个过程结束后**不需要任何额外的微调 (fine-tuning)** 步骤，大大节省了时间和计算资源。
   - **即插即用**：该方法非常通用，可以轻松地集成到各种深度学习应用中，用户几乎不需要任何额外的工程改造。

------



### ## 两大关键技术创新



为了实现上述目标，OTOv2 提出了两个主要的底层技术创新：

1. **自治性 (Autonomy)：自动分析网络并分组**
   - OTOv2 能自动分析任何神经网络的复杂结构，并理解其内部的依赖关系。
   - 它提出了一个关键概念 **“零不变组” (Zero-Invariant Groups, ZIGs)**。简单来说，它能智能地识别出哪些参数是“捆绑”在一起的，必须作为一个整体被同时剪掉或保留，否则网络结构就会被破坏。这使得 OTOv2 能够自动处理各种复杂的模型，而无需人工干预。
2. **双半空间投影梯度 (DHSPG)：全新的高效优化器**
   - 作者设计了一个全新的优化器 DHSPG。这是一个专门为解决结构化稀疏问题而生的强大数学工具。
   - 它能够更稳定、更可靠地在“只训练一次”的过程中，同时完成模型训练和找到最优的稀疏结构这两项任务。

------



### ## 实验结果与意义



- **超强的通用性**：OTOv2 在各种简单和复杂的网络架构上（如 VGG, ResNet, ConvNeXt, DenseNet 等）都展示了出色的效果，而大多数其他方法在处理这些复杂模型时都需要大量的手动改造。
- **顶尖的性能**：在多个基准数据集上的实验表明，OTOv2 的压缩效果与当前最先进的（State-of-the-art）复杂方法相当，甚至在某些情况下表现更好。

**总而言之，OTOv2 的最大贡献在于，它用一个极其简单、自动化的“一站式”流程，取代了过去复杂、繁琐、需要专家介入的多阶段剪枝方法，同时还能在更广泛的模型上达到顶尖的性能，极大地降低了模型压缩技术的使用门槛。** 🛠️✨







[Pruning Deep Neural Networks from a Sparsity Perspective](https://openreview.net/forum?id=i-DleYh34BM)

根据这篇摘要，这篇文章的主要贡献可以概括如下：

------



### ## 核心问题



神经网络剪枝技术虽然发展迅速，但存在一个关键的盲点：在剪枝过程中，研究人员**缺乏一个量化的指标**来判断一个网络还能被“压缩”多少。

这就像医生做手术，虽然知道要切除病变组织，但手边没有生命体征监护仪。全凭经验操作，很容易出现两种问题：

- **剪枝不足 (under-pruning)**：切得太保守，模型依然臃肿。
- **剪枝过度 (over-pruning)**：切得太狠，损伤了模型的“正常功能”，导致性能严重下降。

------



### ## 本文的核心贡献



为了解决这个难题，本文提出了一系列创新，将剪枝从“凭感觉”的艺术，向量化、可指导的科学。

1. **发明了一个“可压缩性”监护仪：PQ 指数 (PQI)**
   - 这是本文最核心的贡献。作者提出了一个全新的量化指标 **PQ Index (PQI)**。
   - **作用**：PQI 就像一台监护仪，可以实时测量一个神经网络在当前状态下的**“潜在可压缩性”**。它能告诉我们，这个网络是否还有继续剪枝的空间，以及何时应该停止。
2. **揭示了剪枝过程中的规律**
   - 通过观察 PQI 在整个剪枝过程中的变化，作者发现了一个清晰的规律性模式：
     - **阶段一（有效压缩）**：刚开始剪枝时，PQI 会**下降**。这表示剪枝正在有效地去除冗余，模型变得更精简，这是一个好信号。
     - **阶段二（达到极限）**：当剪枝达到一个临界点时，PQI 会开始**上升**。这是一个**警报信号**，表明网络的可压缩性已到极限，再剪下去模型就要开始“欠拟合”（变得过于简单而性能下降）了。
     - **阶段三（模型崩溃）**：如果无视警报继续剪枝，PQI 会**再次下降**，但这代表着模型结构已经崩溃，性能开始急剧恶化。
   - 这一发现为剪枝提供了一个清晰的“路线图”，让我们知道何时该“刹车”。
3. **设计了更智能的自适应剪枝算法：SAP**
   - 基于 PQI 这个强大的监护仪，作者开发了一种新的剪枝算法，名为 **SAP (稀疏性感知的自适应剪枝)**。
   - **工作原理**：这个算法会**实时监控 PQI 的值**，并根据其反馈**自适应地调整剪枝的强度**。当 PQI 显示还有很大压缩空间时，它就大胆地剪；当 PQI 发出警报时，它就变得保守或停止剪枝。

------



### ## 实验结果与意义



- 实验证明，作者提出的 SAP 算法在**压缩效率**和**鲁棒性**方面，均**优于**当前流行的迭代式剪枝方法（例如基于“彩票假说”的方法）。
- **总结**：这篇文章最大的贡献是提供了一个**量化剪枝过程的工具 (PQI)**，它不仅帮助我们更深刻地理解了剪枝的动态变化，还催生了一个更智能、更高效的**自适应剪枝算法 (SAP)**，有效避免了剪枝不足或过度的问题。 💡







[How I Learned to Stop Worrying and Love Retraining](https://openreview.net/forum?id=_nF5imFKQI)





